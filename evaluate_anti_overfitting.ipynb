{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Anti-Overfitting Model and Generate All Graphs\n",
    "\n",
    "Load the saved model and test on different data subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset, random_split, Subset\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    confusion_matrix, roc_auc_score, roc_curve, classification_report,\n",
    "    log_loss\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transforms and DWT Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms (same as training)\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# DWT feature extraction\n",
    "def dwt_features(img, wavelet='db1'):\n",
    "    img = np.array(img)\n",
    "    dwt_channels = []\n",
    "    for ch in range(3):\n",
    "        cA, (cH, cV, cD) = pywt.dwt2(img[..., ch], wavelet)\n",
    "        cA_resized = np.array(Image.fromarray(cA).resize((224, 224), Image.BILINEAR))\n",
    "        dwt_channels.append(cA_resized)\n",
    "    dwt_img = np.stack(dwt_channels, axis=2)\n",
    "    return Image.fromarray(dwt_img.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, root_dir, label, transform_rgb, transform_dwt):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform_rgb = transform_rgb\n",
    "        self.transform_dwt = transform_dwt\n",
    "        self.images = [f for f in os.listdir(root_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        self.label = label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_path = os.path.join(self.root_dir, self.images[idx])\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.convert('RGB')\n",
    "                img_np = np.array(img)\n",
    "                \n",
    "                rgb_tensor = self.transform_rgb(image=img_np)['image']\n",
    "                \n",
    "                dwt_img = dwt_features(Image.fromarray(img_np))\n",
    "                dwt_tensor = self.transform_dwt(image=np.array(dwt_img))['image']\n",
    "                \n",
    "                return rgb_tensor, dwt_tensor, self.label\n",
    "        except Exception as e:\n",
    "            dummy_tensor = torch.zeros(3, 224, 224)\n",
    "            return dummy_tensor, dummy_tensor, self.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anti-Overfitting Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AntiOverfitTwoBranchNet(nn.Module):\n",
    "    def __init__(self, n_classes=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # RGB branch\n",
    "        self.rgb_backbone = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        self.rgb_backbone.classifier = nn.Identity()\n",
    "        \n",
    "        # DWT branch\n",
    "        self.dwt_backbone = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        self.dwt_backbone.classifier = nn.Identity()\n",
    "        \n",
    "        # Regularized classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1280*2, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, rgb, dwt):\n",
    "        f1 = F.adaptive_avg_pool2d(self.rgb_backbone.features(rgb), 1).flatten(1)\n",
    "        f2 = F.adaptive_avg_pool2d(self.dwt_backbone.features(dwt), 1).flatten(1)\n",
    "        x = torch.cat([f1, f2], dim=1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device, desc=\"Evaluating\"):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for rgb, dwt, labels in tqdm(data_loader, desc=desc, leave=False):\n",
    "            rgb, dwt, labels = rgb.to(device), dwt.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(rgb, dwt)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    return all_preds, all_labels, all_probs, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, y_probs):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    if len(np.unique(y_true)) > 1 and not np.any(np.isnan(y_probs)):\n",
    "        auc = roc_auc_score(y_true, y_probs[:, 1])\n",
    "        entropy = log_loss(y_true, y_probs)\n",
    "    else:\n",
    "        auc = 0.5\n",
    "        entropy = 1.0\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc_score': auc,\n",
    "        'entropy': entropy,\n",
    "        'confusion_matrix': cm\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_results(checkpoint_data, test_results, val_results, holdout_results):\n",
    "    \"\"\"Generate all graphs and analysis\"\"\"\n",
    "    # Extract training history\n",
    "    train_losses = checkpoint_data.get('train_losses', [])\n",
    "    val_losses = checkpoint_data.get('val_losses', [])\n",
    "    train_accs = checkpoint_data.get('train_accs', [])\n",
    "    val_accs = checkpoint_data.get('val_accs', [])\n",
    "    \n",
    "    # Create comprehensive figure\n",
    "    fig = plt.figure(figsize=(24, 16))\n",
    "    \n",
    "    # Training Loss and Accuracy\n",
    "    ax1 = plt.subplot(3, 4, 1)\n",
    "    if train_losses and val_losses:\n",
    "        plt.plot(train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "        plt.plot(val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "        plt.title('Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2 = plt.subplot(3, 4, 2)\n",
    "    if train_accs and val_accs:\n",
    "        plt.plot(train_accs, 'b-', label='Training Accuracy', linewidth=2)\n",
    "        plt.plot(val_accs, 'r-', label='Validation Accuracy', linewidth=2)\n",
    "        plt.axhline(y=0.9, color='green', linestyle='--', alpha=0.7, label='90% Target')\n",
    "        plt.title('Training & Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Overfitting Analysis\n",
    "    ax3 = plt.subplot(3, 4, 3)\n",
    "    if train_accs and val_accs:\n",
    "        gap = [abs(t - v) for t, v in zip(train_accs, val_accs)]\n",
    "        plt.plot(gap, 'purple', linewidth=2, label='Train-Val Gap')\n",
    "        plt.axhline(y=0.05, color='red', linestyle='--', alpha=0.7, label='Overfitting Threshold')\n",
    "        plt.title('Overfitting Analysis', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy Gap')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Test Set Confusion Matrix\n",
    "    ax4 = plt.subplot(3, 4, 4)\n",
    "    sns.heatmap(test_results['confusion_matrix'], annot=True, fmt='d', cmap='Blues', ax=ax4)\n",
    "    ax4.set_title('Test Set Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('Predicted')\n",
    "    ax4.set_ylabel('Actual')\n",
    "    ax4.set_xticklabels(['Original', 'Deepfake'])\n",
    "    ax4.set_yticklabels(['Original', 'Deepfake'])\n",
    "    \n",
    "    # ROC Curves Comparison\n",
    "    ax5 = plt.subplot(3, 4, 5)\n",
    "    # Test set ROC\n",
    "    if len(np.unique(test_results['labels'])) > 1:\n",
    "        fpr, tpr, _ = roc_curve(test_results['labels'], test_results['probabilities'][:, 1])\n",
    "        plt.plot(fpr, tpr, color='blue', lw=2, label=f'Test Set (AUC = {test_results[\"auc_score\"]:.3f})')\n",
    "    \n",
    "    # Validation set ROC\n",
    "    if len(np.unique(val_results['labels'])) > 1:\n",
    "        fpr, tpr, _ = roc_curve(val_results['labels'], val_results['probabilities'][:, 1])\n",
    "        plt.plot(fpr, tpr, color='red', lw=2, label=f'Validation Set (AUC = {val_results[\"auc_score\"]:.3f})')\n",
    "    \n",
    "    # Holdout set ROC\n",
    "    if len(np.unique(holdout_results['labels'])) > 1:\n",
    "        fpr, tpr, _ = roc_curve(holdout_results['labels'], holdout_results['probabilities'][:, 1])\n",
    "        plt.plot(fpr, tpr, color='green', lw=2, label=f'Holdout Set (AUC = {holdout_results[\"auc_score\"]:.3f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Performance Comparison\n",
    "    ax6 = plt.subplot(3, 4, 6)\n",
    "    datasets = ['Test', 'Validation', 'Holdout']\n",
    "    accuracies = [test_results['accuracy'], val_results['accuracy'], holdout_results['accuracy']]\n",
    "    colors = ['blue', 'red', 'green']\n",
    "    bars = plt.bar(datasets, accuracies, color=colors, alpha=0.7)\n",
    "    plt.title('Accuracy Comparison Across Datasets', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, accuracies):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Metrics Comparison\n",
    "    ax7 = plt.subplot(3, 4, 7)\n",
    "    metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\n",
    "    test_values = [test_results['accuracy'], test_results['precision'], \n",
    "                   test_results['recall'], test_results['f1_score'], test_results['auc_score']]\n",
    "    \n",
    "    x = np.arange(len(metrics_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    plt.bar(x - width, test_values, width, label='Test Set', color='blue', alpha=0.7)\n",
    "    plt.bar(x, [val_results['accuracy'], val_results['precision'], val_results['recall'], \n",
    "               val_results['f1_score'], val_results['auc_score']], \n",
    "            width, label='Validation', color='red', alpha=0.7)\n",
    "    plt.bar(x + width, [holdout_results['accuracy'], holdout_results['precision'], \n",
    "                       holdout_results['recall'], holdout_results['f1_score'], \n",
    "                       holdout_results['auc_score']], \n",
    "            width, label='Holdout', color='green', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Comprehensive Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(x, metrics_names, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim([0, 1])\n",
    "    \n",
    "    # Prediction Distribution\n",
    "    ax8 = plt.subplot(3, 4, 8)\n",
    "    plt.hist(test_results['probabilities'][:, 1], bins=50, alpha=0.5, color='blue', \n",
    "             label='Test Set', density=True)\n",
    "    plt.hist(val_results['probabilities'][:, 1], bins=50, alpha=0.5, color='red', \n",
    "             label='Validation', density=True)\n",
    "    plt.hist(holdout_results['probabilities'][:, 1], bins=50, alpha=0.5, color='green', \n",
    "             label='Holdout', density=True)\n",
    "    plt.axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "    plt.title('Prediction Probability Distribution', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Deepfake Probability')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Model Summary\n",
    "    ax9 = plt.subplot(3, 4, 9)\n",
    "    summary_text = f\"\"\"MODEL PERFORMANCE SUMMARY\n",
    "\n",
    "Test Set:\n",
    "â€¢ Accuracy: {test_results['accuracy']:.4f} ({test_results['accuracy']*100:.2f}%)\n",
    "â€¢ Precision: {test_results['precision']:.4f}\n",
    "â€¢ Recall: {test_results['recall']:.4f}\n",
    "â€¢ F1-Score: {test_results['f1_score']:.4f}\n",
    "â€¢ AUC: {test_results['auc_score']:.4f}\n",
    "\n",
    "Validation Set:\n",
    "â€¢ Accuracy: {val_results['accuracy']:.4f} ({val_results['accuracy']*100:.2f}%)\n",
    "â€¢ AUC: {val_results['auc_score']:.4f}\n",
    "\n",
    "Holdout Set:\n",
    "â€¢ Accuracy: {holdout_results['accuracy']:.4f} ({holdout_results['accuracy']*100:.2f}%)\n",
    "â€¢ AUC: {holdout_results['auc_score']:.4f}\"\"\"\n",
    "    \n",
    "    plt.text(0.05, 0.95, summary_text, transform=ax9.transAxes, fontsize=10, \n",
    "             verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "    plt.title('Performance Summary', fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Generalization Analysis\n",
    "    ax10 = plt.subplot(3, 4, 10)\n",
    "    if train_accs and val_accs:\n",
    "        final_gap = abs(train_accs[-1] - val_accs[-1])\n",
    "        accuracies = [test_results['accuracy'], val_results['accuracy'], holdout_results['accuracy']]\n",
    "        \n",
    "        generalization_text = f\"\"\"GENERALIZATION ANALYSIS\n",
    "\n",
    "Train-Val Gap: {final_gap:.4f}\n",
    "Status: {\"âœ… Excellent\" if final_gap < 0.02 else \"âœ… Good\" if final_gap < 0.05 else \"âš  Moderate\" if final_gap < 0.1 else \"âŒ Poor\"}\n",
    "\n",
    "Cross-Dataset Performance:\n",
    "â€¢ Test: {test_results['accuracy']*100:.1f}%\n",
    "â€¢ Validation: {val_results['accuracy']*100:.1f}%  \n",
    "â€¢ Holdout: {holdout_results['accuracy']*100:.1f}%\n",
    "\n",
    "Consistency: {\"âœ… Excellent\" if max(accuracies) - min(accuracies) < 0.02 else \"âœ… Good\" if max(accuracies) - min(accuracies) < 0.05 else \"âš  Moderate\"}\n",
    "\n",
    "Recommendation:\n",
    "{\"ðŸŽ¯ Model generalizes very well!\" if final_gap < 0.02 and max(accuracies) - min(accuracies) < 0.02 else \"âœ… Good generalization\" if final_gap < 0.05 else \"âš  Consider more regularization\"}\"\"\"\n",
    "        \n",
    "        color = \"lightgreen\" if final_gap < 0.02 else \"lightyellow\" if final_gap < 0.05 else \"lightcoral\"\n",
    "        plt.text(0.05, 0.95, generalization_text, transform=ax10.transAxes, fontsize=10, \n",
    "                 verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=color))\n",
    "    \n",
    "    plt.title('Generalization Assessment', fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Training Progress\n",
    "    ax11 = plt.subplot(3, 4, 11)\n",
    "    if train_losses and val_losses:\n",
    "        epochs = range(1, len(train_losses) + 1)\n",
    "        plt.plot(epochs, train_losses, 'b-', label='Train Loss', linewidth=2)\n",
    "        plt.plot(epochs, val_losses, 'r-', label='Val Loss', linewidth=2)\n",
    "        plt.fill_between(epochs, train_losses, val_losses, alpha=0.3, color='gray', label='Overfitting Gap')\n",
    "        plt.title('Training Progress with Gap Analysis', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Final Confusion Matrices Comparison\n",
    "    ax12 = plt.subplot(3, 4, 12)\n",
    "    # Create a combined confusion matrix visualization\n",
    "    fig_cm, axes_cm = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Test CM\n",
    "    sns.heatmap(test_results['confusion_matrix'], annot=True, fmt='d', cmap='Blues', ax=axes_cm[0])\n",
    "    axes_cm[0].set_title('Test Set')\n",
    "    axes_cm[0].set_xlabel('Predicted')\n",
    "    axes_cm[0].set_ylabel('Actual')\n",
    "    \n",
    "    # Val CM\n",
    "    sns.heatmap(val_results['confusion_matrix'], annot=True, fmt='d', cmap='Reds', ax=axes_cm[1])\n",
    "    axes_cm[1].set_title('Validation Set')\n",
    "    axes_cm[1].set_xlabel('Predicted')\n",
    "    axes_cm[1].set_ylabel('Actual')\n",
    "    \n",
    "    # Holdout CM\n",
    "    sns.heatmap(holdout_results['confusion_matrix'], annot=True, fmt='d', cmap='Greens', ax=axes_cm[2])\n",
    "    axes_cm[2].set_title('Holdout Set')\n",
    "    axes_cm[2].set_xlabel('Predicted')\n",
    "    axes_cm[2].set_ylabel('Actual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrices_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig_cm)\n",
    "    \n",
    "    # Remove the 12th subplot and add text\n",
    "    ax12.text(0.5, 0.5, 'Detailed Confusion Matrices\\nsaved as separate file:\\nconfusion_matrices_comparison.png', \n",
    "              transform=ax12.transAxes, fontsize=12, ha='center', va='center',\n",
    "              bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
    "    ax12.set_title('Additional Analysis', fontsize=14, fontweight='bold')\n",
    "    ax12.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('complete_anti_overfitting_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ” Anti-Overfitting Model Evaluation & Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if model exists\n",
    "model_path = \"anti_overfitting_model.pth\"\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"âŒ Model file not found: {model_path}\")\n",
    "    print(\"Please run the training script first!\")\n",
    "else:\n",
    "    print(f\"âœ… Model file found: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and checkpoint data\n",
    "print(\"ðŸ“‚ Loading trained model...\")\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model = AntiOverfitTwoBranchNet(n_classes=2).to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"âœ… Model loaded successfully!\")\n",
    "print(f\"ðŸ“Š Training completed at epoch: {checkpoint.get('epoch', 'Unknown')}\")\n",
    "print(f\"ðŸ† Best validation loss: {checkpoint.get('best_val_loss', 'Unknown'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "deepfake_dir = \"deeplearning/Deepfake/extracted_faces\"\n",
    "original_dir = \"deeplearning/Original/extracted_faces\"\n",
    "\n",
    "if not os.path.exists(deepfake_dir) or not os.path.exists(original_dir):\n",
    "    print(\"âŒ Dataset directories not found!\")\n",
    "    print(f\"Looking for: {deepfake_dir}\")\n",
    "    print(f\"Looking for: {original_dir}\")\n",
    "else:\n",
    "    print(f\"âœ… Dataset directories found\")\n",
    "    print(f\"  - Deepfake: {deepfake_dir}\")\n",
    "    print(f\"  - Original: {original_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "print(\"\\nðŸ“‚ Loading datasets for comprehensive evaluation...\")\n",
    "deepfake_dataset = EvalDataset(deepfake_dir, label=1, transform_rgb=val_transform, transform_dwt=val_transform)\n",
    "original_dataset = EvalDataset(original_dir, label=0, transform_rgb=val_transform, transform_dwt=val_transform)\n",
    "all_dataset = ConcatDataset([deepfake_dataset, original_dataset])\n",
    "\n",
    "print(f\"ðŸ“Š Total dataset size: {len(all_dataset):,} samples\")\n",
    "print(f\"  - Deepfake samples: {len(deepfake_dataset):,}\")\n",
    "print(f\"  - Original samples: {len(original_dataset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different test splits\n",
    "print(\"ðŸ”„ Creating different data splits for robust evaluation...\")\n",
    "\n",
    "# Original splits (same as training)\n",
    "train_size = int(0.7 * len(all_dataset))\n",
    "val_size = int(0.2 * len(all_dataset))\n",
    "test_size = len(all_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    all_dataset, [train_size, val_size, test_size], \n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Create a completely different holdout set (different random seed)\n",
    "holdout_size = min(30000, len(all_dataset) // 20)  # 5% of data\n",
    "holdout_indices = torch.randperm(len(all_dataset), generator=torch.Generator().manual_seed(123))[:holdout_size]\n",
    "holdout_dataset = Subset(all_dataset, holdout_indices)\n",
    "\n",
    "print(f\"ðŸ“Š Evaluation datasets:\")\n",
    "print(f\"  - Test set: {len(test_dataset):,} samples\")\n",
    "print(f\"  - Validation set: {len(val_dataset):,} samples\")\n",
    "print(f\"  - Holdout set: {len(holdout_dataset):,} samples (different split)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 32\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "holdout_loader = DataLoader(holdout_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"âœ… Data loaders created with batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Comprehensive Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ§ª Running comprehensive evaluation...\")\n",
    "\n",
    "print(\"  ðŸ“Š Evaluating on test set...\")\n",
    "test_preds, test_labels, test_probs, test_loss = evaluate_model(\n",
    "    model, test_loader, criterion, device, \"Test Set\"\n",
    ")\n",
    "test_results = calculate_metrics(test_labels, test_preds, test_probs)\n",
    "test_results['labels'] = test_labels\n",
    "test_results['predictions'] = test_preds\n",
    "test_results['probabilities'] = test_probs\n",
    "\n",
    "print(f\"âœ… Test set evaluation completed - Accuracy: {test_results['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  ðŸ“Š Evaluating on validation set...\")\n",
    "val_preds, val_labels, val_probs, val_loss = evaluate_model(\n",
    "    model, val_loader, criterion, device, \"Validation Set\"\n",
    ")\n",
    "val_results = calculate_metrics(val_labels, val_preds, val_probs)\n",
    "val_results['labels'] = val_labels\n",
    "val_results['predictions'] = val_preds\n",
    "val_results['probabilities'] = val_probs\n",
    "\n",
    "print(f\"âœ… Validation set evaluation completed - Accuracy: {val_results['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  ðŸ“Š Evaluating on holdout set...\")\n",
    "holdout_preds, holdout_labels, holdout_probs, holdout_loss = evaluate_model(\n",
    "    model, holdout_loader, criterion, device, \"Holdout Set\"\n",
    ")\n",
    "holdout_results = calculate_metrics(holdout_labels, holdout_preds, holdout_probs)\n",
    "holdout_results['labels'] = holdout_labels\n",
    "holdout_results['predictions'] = holdout_preds\n",
    "holdout_results['probabilities'] = holdout_probs\n",
    "\n",
    "print(f\"âœ… Holdout set evaluation completed - Accuracy: {holdout_results['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comprehensive results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ˆ COMPREHENSIVE ANTI-OVERFITTING EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ TEST SET RESULTS:\")\n",
    "print(f\"  âœ… Accuracy:  {test_results['accuracy']:.4f} ({test_results['accuracy']*100:.2f}%)\")\n",
    "print(f\"  âœ… Precision: {test_results['precision']:.4f}\")\n",
    "print(f\"  âœ… Recall:    {test_results['recall']:.4f}\")\n",
    "print(f\"  âœ… F1-Score:  {test_results['f1_score']:.4f}\")\n",
    "print(f\"  âœ… AUC Score: {test_results['auc_score']:.4f}\")\n",
    "print(f\"  âœ… Entropy:   {test_results['entropy']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ VALIDATION SET RESULTS:\")\n",
    "print(f\"  âœ… Accuracy:  {val_results['accuracy']:.4f} ({val_results['accuracy']*100:.2f}%)\")\n",
    "print(f\"  âœ… AUC Score: {val_results['auc_score']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ HOLDOUT SET RESULTS (Different Data Split):\")\n",
    "print(f\"  âœ… Accuracy:  {holdout_results['accuracy']:.4f} ({holdout_results['accuracy']*100:.2f}%)\")\n",
    "print(f\"  âœ… AUC Score: {holdout_results['auc_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalization analysis\n",
    "accuracies = [test_results['accuracy'], val_results['accuracy'], holdout_results['accuracy']]\n",
    "consistency = max(accuracies) - min(accuracies)\n",
    "\n",
    "print(f\"\\nðŸ” GENERALIZATION ANALYSIS:\")\n",
    "print(f\"  ðŸ“Š Accuracy Range: {min(accuracies)*100:.2f}% - {max(accuracies)*100:.2f}%\")\n",
    "print(f\"  ðŸ“Š Consistency Gap: {consistency:.4f}\")\n",
    "\n",
    "if consistency < 0.02:\n",
    "    print(f\"  âœ… EXCELLENT: Model shows excellent generalization across different datasets!\")\n",
    "elif consistency < 0.05:\n",
    "    print(f\"  âœ… GOOD: Model shows good generalization\")\n",
    "else:\n",
    "    print(f\"  âš   MODERATE: Some variation across datasets\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Comprehensive Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“ˆ Generating comprehensive visualization...\")\n",
    "plot_all_results(checkpoint, test_results, val_results, holdout_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“‹ DETAILED CLASSIFICATION REPORTS:\")\n",
    "print(\"\\nðŸŽ¯ Test Set:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=['Original', 'Deepfake'], zero_division=0))\n",
    "\n",
    "print(\"\\nðŸŽ¯ Holdout Set:\")\n",
    "print(classification_report(holdout_labels, holdout_preds, target_names=['Original', 'Deepfake'], zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "\n",
    "print(f\"\\nðŸ† FINAL SUMMARY:\")\n",
    "print(f\"  ðŸ“Š Average Accuracy Across All Sets: {avg_accuracy:.4f} ({avg_accuracy*100:.2f}%)\")\n",
    "print(f\"  ðŸ’¾ Model saved as: {model_path}\")\n",
    "print(f\"  ðŸ“Š Complete analysis saved as: complete_anti_overfitting_analysis.png\")\n",
    "print(f\"  ðŸ“Š Confusion matrices saved as: confusion_matrices_comparison.png\")\n",
    "\n",
    "if avg_accuracy >= 0.85 and consistency < 0.05:\n",
    "    print(f\"\\nðŸŽ¯ SUCCESS! Model achieves excellent performance with good generalization!\")\n",
    "    print(\"ðŸ† Anti-overfitting training was successful!\")\n",
    "elif avg_accuracy >= 0.80:\n",
    "    print(f\"\\nâœ… Good performance with reasonable generalization\")\n",
    "else:\n",
    "    print(f\"\\nðŸ“Š Model shows room for improvement\")\n",
    "\n",
    "print(\"\\nâœ… Comprehensive evaluation completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}